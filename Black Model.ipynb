{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data generation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\serpo\\Documents\\Python\\Black\\.venv\\Lib\\site-packages\\torch\\_subclasses\\functional_tensor.py:258: UserWarning: Failed to initialize NumPy: No module named 'numpy.core._exceptions' (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def black_model(F, K, T, sigma, option_type='call'):\n",
    "\t# Parameters\n",
    "\td1 = (np.log(F / K) + (sigma**2 / 2) * T) / (sigma * np.sqrt(T))\n",
    "\td2 = d1 - sigma * np.sqrt(T)\n",
    "\t\n",
    "\t# Discount factor (assuming risk-free rate is 0)\n",
    "\tDF_T = 1\n",
    "\t\n",
    "\tif option_type == 'call':\n",
    "\t\treturn DF_T * (F * norm.cdf(d1) - K * norm.cdf(d2))\n",
    "\telif option_type == 'put':\n",
    "\t\treturn DF_T * (K * norm.cdf(-d2) - F * norm.cdf(-d1))\n",
    "\telse:\n",
    "\t\traise ValueError(\"option_type must be 'call' or 'put'\")\n",
    "\n",
    "def generate_data(num_samples, S=1):\n",
    "\t# Generate random parameters\n",
    "\tK = np.random.uniform(1, 2.5, num_samples)\n",
    "\tT = np.random.uniform(0.004, 4, num_samples)\n",
    "\tsigma = np.random.uniform(0.1, 0.5, num_samples)\n",
    "\n",
    "\tcall_prices = black_model(S, K, T, sigma, option_type='call')\n",
    "\t\n",
    "\t# Prepare input data matrix X\n",
    "\tX = np.vstack((K, T, np.log(K), sigma * np.sqrt(T), sigma**2 * T)).T\n",
    "\ty = call_prices\n",
    "\t\n",
    "\treturn X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_static_test_data(num_test_samples=100000, file_name='static_test_data.pt'):\n",
    "    X_test, y_test = generate_data(num_samples=num_test_samples)\n",
    "    \n",
    "    # Преобразуем в тензоры\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float64)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float64).unsqueeze(1)\n",
    "\n",
    "    # Сохраняем данные\n",
    "    torch.save((X_test_tensor, y_test_tensor), file_name)\n",
    "\n",
    "def load_static_test_data(file_name='static_test_data.pt'):\n",
    "\t# Загружаем данные из файла\n",
    "\tX_test_tensor, y_test_tensor = torch.load(file_name)\n",
    "\n",
    "\treturn X_test_tensor, y_test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_prepare_training_data(num_train_samples=1000000, batch_size=128):\n",
    "    X_train, y_train = generate_data(num_samples=num_train_samples)\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float64)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float64).unsqueeze(1)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Train**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelAttentionLayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(KernelAttentionLayer, self).__init__()\n",
    "        self.query_layer = nn.Linear(input_size, output_size, dtype=torch.float64)\n",
    "        self.key_layer = nn.Linear(input_size, output_size, dtype=torch.float64)\n",
    "        self.value_layer = nn.Linear(input_size, output_size, dtype=torch.float64)\n",
    "        self.scale = 1.0 / np.sqrt(output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        Q = self.query_layer(x)\n",
    "        K = self.key_layer(x)\n",
    "        V = self.value_layer(x)\n",
    "        \n",
    "        attention_weights = torch.softmax(Q @ K.T * self.scale, dim=-1)\n",
    "        output = attention_weights @ V\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlackScholesNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(BlackScholesNet, self).__init__()\n",
    "        self.kernel_attention = KernelAttentionLayer(input_size, hidden_size)\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size, dtype=torch.float64)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1, dtype=torch.float64)\n",
    "        self.name = 'KAN Black Scholes'\n",
    "    \n",
    "    def forward(self, x):\t\n",
    "        x = self.kernel_attention(x)\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        output = self.fc2(x)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlackScholesNet(nn.Module):\n",
    "\tdef __init__(self, input_size=1, hidden_size=128, output_size=1, dropout_p = 0.33):\n",
    "\t\tsuper(BlackScholesNet, self).__init__()\n",
    "\t\tself.fc1 = nn.Linear(input_size, hidden_size, dtype=torch.float64)\n",
    "\t\tself.bn1 = nn.BatchNorm1d(hidden_size, dtype=torch.float64)  # Batch Normalization\n",
    "\t\tself.fc2 = nn.Linear(hidden_size, hidden_size, dtype=torch.float64)\n",
    "\t\tself.bn2 = nn.BatchNorm1d(hidden_size, dtype=torch.float64)  # Batch Normalization\n",
    "\t\tself.fc3 = nn.Linear(hidden_size, output_size, dtype=torch.float64)\n",
    "\t\tself.dropout = nn.Dropout(p=dropout_p)  # Dropout for regularization\n",
    "\t\tself.name = 'Black Model'\n",
    "\n",
    "\tdef forward(self, x, K):\n",
    "\t\tx = F.tanh(self.bn1(self.fc1(x)))  # Tanh and Batch Normalization\n",
    "\t\tx = self.dropout(x)  # Dropout\n",
    "\t\tx = F.tanh(self.bn2(self.fc2(x)))  # Tanh and Batch Normalization\n",
    "\t\tx = self.fc3(x)\n",
    "\t\tx1, x2 = x[:, [0]], x[:, [1]]\n",
    "\t\treturn F.sigmoid(x1) - K * F.sigmoid(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlackScholesNet(nn.Module):\n",
    "\tdef __init__(self, input_size=1, hidden_size=128, output_size=1, dropout_p = 0.33):\n",
    "\t\tsuper(BlackScholesNet, self).__init__()\n",
    "\t\tself.fc1 = nn.Linear(input_size, hidden_size, dtype=torch.float64)\n",
    "\t\tself.bn1 = nn.BatchNorm1d(hidden_size, dtype=torch.float64)  # Batch Normalization\n",
    "\t\tself.fc2 = nn.Linear(hidden_size, hidden_size, dtype=torch.float64)\n",
    "\t\tself.bn2 = nn.BatchNorm1d(hidden_size, dtype=torch.float64)  # Batch Normalization\n",
    "\t\tself.fc3 = nn.Linear(hidden_size, output_size, dtype=torch.float64)\n",
    "\t\tself.dropout = nn.Dropout(p=dropout_p)  # Dropout for regularization\n",
    "\t\tself.name = 'Black Model without activation'\n",
    "\n",
    "\tdef forward(self, x, K):\n",
    "\t\tx = F.tanh(self.bn1(self.fc1(x)))  # Tanh and Batch Normalization\n",
    "\t\tx = self.dropout(x)  # Dropout\n",
    "\t\tx = F.tanh(self.bn2(self.fc2(x)))  # Tanh and Batch Normalization\n",
    "\t\tx = self.fc3(x)\n",
    "\t\treturn x\n",
    "\t\t# x1, x2 = x[:, [0]], x[:, [1]]\n",
    "\t\t# return F.sigmoid(x1) - K * F.sigmoid(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlackScholesNet(nn.Module):\n",
    "\tdef __init__(self, input_size=1, hidden_size=128, output_size=1, dropout_p=0.3):\n",
    "\t\tsuper(BlackScholesNet, self).__init__()\n",
    "\t\tself.fc1 = nn.Linear(input_size, hidden_size, dtype=torch.float64)\n",
    "\t\tself.bn1 = nn.BatchNorm1d(hidden_size, dtype=torch.float64)\n",
    "\t\tself.fc2 = nn.Linear(hidden_size, hidden_size, dtype=torch.float64)\n",
    "\t\tself.bn2 = nn.BatchNorm1d(hidden_size, dtype=torch.float64)\n",
    "\t\tself.fc_out = nn.Linear(hidden_size, output_size, dtype=torch.float64)\n",
    "\t\tself.dropout = nn.Dropout(p=dropout_p)\n",
    "\t\tself.name = 'Black ResNet Model'\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tresidual = self.fc1(x)  # Transform residual to match shape of x\n",
    "\t\tx = F.relu(self.bn1(self.fc1(x)))\n",
    "\t\tx = self.dropout(x)\n",
    "\t\tx = F.relu(self.bn2(self.fc2(x)))\n",
    "\t\tx = x + residual  # Adding the skip connection after matching dimensions\n",
    "\t\tx = self.fc_out(x)\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlackScholesNet(nn.Module):\n",
    "\tdef __init__(self, input_size=1, hidden_size=128, output_size=1, num_layers=2, dropout_p=0.3):\n",
    "\t\tsuper(BlackScholesNet, self).__init__()\n",
    "\t\tself.rnn = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True, dtype=torch.float64)\n",
    "\t\tself.fc = nn.Linear(hidden_size, output_size, dtype=torch.float64)\n",
    "\t\tself.dropout = nn.Dropout(p=dropout_p)\n",
    "\t\tself.name = 'Black RNN Model'\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx, _ = self.rnn(x)\n",
    "\t\tx = self.fc(x[:, -1, :])  # Take output of the last time step\n",
    "\t\treturn  F.tanh(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_samples = 500000\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_static_test_data(num_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tensor, y_test_tensor = load_static_test_data()\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding optimal hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = generate_and_prepare_training_data(1000000, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tensor, y_test_tensor = load_static_test_data()\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def objective(trial):\n",
    "\t# Define hyperparameters to be optimized\n",
    "\tinput_size = 5\n",
    "\thidden_size = 128\n",
    "\t# dropout_p = trial.suggest_float('dropout_p', 0.1, 0.5)\n",
    "\t# lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n",
    "\tdropout_p = 0.20862014926048447\n",
    "\tlr = 0.0002448376394581503\n",
    "\t\n",
    "\tmodel = BlackScholesNet(input_size=input_size, hidden_size=hidden_size, output_size=2)\n",
    "\tmodel.dropout.p = dropout_p\n",
    "\n",
    "\toptimizer = torch.optim.NAdam(model.parameters(), lr=lr)\n",
    "\t\n",
    "\tscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)\n",
    "\n",
    "\tnum_epochs = 30\n",
    "\tfor epoch in range(num_epochs):\n",
    "\t\tmodel.train()\n",
    "\t\tepoch_mse = 0\n",
    "\t\tepoch_mae = 0\n",
    "\t\tepoch_mre = 0\n",
    "\t\t\n",
    "\t\tfor X_batch, y_batch in train_loader:\n",
    "\t\t\tX_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\t\t\t\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\t\n",
    "\t\t\toutputs = model(X_batch, X_batch[:, 0])\n",
    "\t\t\toutputs = (outputs[:, 0] + outputs[:, 1]) / 2\n",
    "\t\t\ty = y_batch[:, 0]\n",
    "\t\t\t\n",
    "\t\t\t# mse_loss = F.mse_loss(outputs, y)\n",
    "\t\t\tmae_loss = F.l1_loss(outputs, y)\n",
    "\t\t\t# relative_errors = torch.abs(outputs - y) / (y + 1e-8)\n",
    "\t\t\t# mre_loss = relative_errors.mean()\n",
    "\t\t\t\n",
    "\t\t\tmae_loss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\t\n",
    "\t\t\t# epoch_mse += mse_loss.item()\n",
    "\t\t\tepoch_mae += mae_loss.item()\n",
    "\t\t\t# epoch_mre += mre_loss.item()\n",
    "\t\t\n",
    "\t\tavg_epoch_mae = epoch_mae / len(train_loader)\n",
    "\t\tscheduler.step(avg_epoch_mae)\n",
    "\t\t\n",
    "\ttest_losses = 0.\n",
    "\ttest_maes = 0.\n",
    "\ttest_max_aes = 0.\n",
    "\ttest_mres = 0.\n",
    "\ttest_max_res = 0.\n",
    "\n",
    "\tmodel.eval()\n",
    "\n",
    "\twith torch.inference_mode():\n",
    "\t\tfor X_batch, y_batch in test_loader:\n",
    "\t\t\tX_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "\t\t\t# Forward pass\n",
    "\t\t\toutputs = model(X_batch, X_batch[:, 0])\n",
    "\t\t\toutputs = (outputs[:, 0] + outputs[:, 1]) / 2\n",
    "\t\t\t\n",
    "\t\t\ty = y_batch[:, 0]\n",
    "\n",
    "\t\t\t# Mean Squared Error (MSE)\n",
    "\t\t\t# mse_loss = F.mse_loss(outputs, y)\n",
    "\t\t\t# test_losses += mse_loss.item()\n",
    "\n",
    "\t\t\t# Mean Absolute Error (MAE)\n",
    "\t\t\tabs_errors = torch.abs(outputs - y)\n",
    "\t\t\ttest_maes += abs_errors.sum().item()\n",
    "\n",
    "\t\t\t# # Maximum Absolute Error (Max AE)\n",
    "\t\t\t# max_ae = abs_errors.max().item()\n",
    "\t\t\t# test_max_aes = max(test_max_aes, max_ae)\n",
    "\n",
    "\t\t\t# # Mean Relative Error (MRE)\n",
    "\t\t\t# mask = y == 0\n",
    "\t\t\t# zero_price_mre = abs_errors[mask]\n",
    "\t\t\t# price_mre = abs_errors[~mask]\n",
    "\n",
    "\t\t\t# # Avoid division by zero for non-zero y values\n",
    "\t\t\t# nonzero_y = y[~mask]\n",
    "\t\t\t# price_mre = price_mre / nonzero_y if nonzero_y.numel() > 0 else price_mre\n",
    "\n",
    "\t\t\t# # Calculate MRE\n",
    "\t\t\t# total_mre = zero_price_mre.sum() + price_mre.sum()\n",
    "\t\t\t# test_mres += total_mre.item() if zero_price_mre.numel() > 0 or price_mre.numel() > 0 else 0\n",
    "\n",
    "\t\t\t# # Handle empty tensors and `inf` values for Max RE\n",
    "\t\t\t# zero_price_max = zero_price_mre.max() if zero_price_mre.numel() > 0 else 0\n",
    "\t\t\t# price_max = price_mre.max() if price_mre.numel() > 0 else 0\n",
    "\n",
    "\t\t\t# # Calculate max relative error\n",
    "\t\t\t# max_re = max(zero_price_max.item(), price_max.item())\n",
    "\t\t\t# test_max_res = max(test_max_res, max_re)\n",
    "\n",
    "\t# avg_test_loss = test_losses / len(test_loader.dataset)\n",
    "\tavg_test_mae = test_maes / len(test_loader.dataset)\n",
    "\t# avg_test_mre = test_mres / len(test_loader.dataset)\n",
    "\t\t\t\n",
    "\t# Return the average MAE as the objective to be minimized\n",
    "\treturn avg_test_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Optuna study and optimize\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Print best parameters and best value\n",
    "print(\"Best hyperparameters:\", study.best_params)\n",
    "print(\"Best MAE:\", study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best hyperparameters: {'dropout_p': 0.20862014926048447, 'lr': 0.0002448376394581503}\n",
    "Best MAE: 0.00049047276004533371"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_p = 0.20862014926048447\n",
    "lr = 0.004448376394581503"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BlackScholesNet(input_size=5, hidden_size=64)\n",
    "# model.dropout.p = dropout_p\n",
    "optimizer = optim.NAdam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('models\\\\Black Model_mae.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BlackScholesNet(\n",
       "  (kernel_attention): KernelAttentionLayer(\n",
       "    (query_layer): Linear(in_features=5, out_features=64, bias=True)\n",
       "    (key_layer): Linear(in_features=5, out_features=64, bias=True)\n",
       "    (value_layer): Linear(in_features=5, out_features=64, bias=True)\n",
       "  )\n",
       "  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best hyperparameters: {'hidden_size': 256, 'dropout_p': 0.16165214075232218, 'lr': 0.0024867405570057574, 'batch_size': 64, 'optimizer': 'NAdam'}\n",
    "Best MAE: 0.003318011008932989"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best hyperparameters: {'hidden_size': 128, 'dropout_p': 0.3974039374569882, 'lr': 0.012408717790861197, 'batch_size': 128, 'optimizer': 'NAdam'}\n",
    "Best MAE: 0.008668262018621945"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_samples = 1000000\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = generate_and_prepare_training_data(num_train_samples, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "num_stages = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "\tif isinstance(m, nn.Linear):\n",
    "\t\tnn.init.xavier_uniform_(m.weight)\n",
    "\t\tif m.bias is not None:\n",
    "\t\t\tnn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BlackScholesNet(\n",
       "  (kernel_attention): KernelAttentionLayer(\n",
       "    (query_layer): Linear(in_features=5, out_features=64, bias=True)\n",
       "    (key_layer): Linear(in_features=5, out_features=64, bias=True)\n",
       "    (value_layer): Linear(in_features=5, out_features=64, bias=True)\n",
       "  )\n",
       "  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huber Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_loss(y_pred, y_true, delta=1.0):\n",
    "\terror = y_true - y_pred\n",
    "\tis_small_error = torch.abs(error) <= delta\n",
    "\tsmall_error_loss = 0.5 * error**2\n",
    "\tlarge_error_loss = delta * (torch.abs(error) - 0.5 * delta)\n",
    "\n",
    "\treturn torch.where(is_small_error, small_error_loss, large_error_loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stage in range(num_stages):\n",
    "\tprint(f\"[ {datetime.now().strftime(\"%H:%M:%S\")} ] ***** Stage [{stage+1}/{num_stages}] {'*'*150}\")\n",
    "\t\n",
    "\tfor epoch in range(num_epochs):\n",
    "\t\tepoch_huber = 0.\n",
    "\t\tepoch_mae = 0.\n",
    "\t\tepoch_mre = 0.\n",
    "\t\tepoch_mse = 0.\n",
    "\t\t\n",
    "\t\tfor X_batch, y_batch in train_loader:\n",
    "\t\t\tX_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\t\t\t# X_batch = X_batch.unsqueeze(1)  # Add a dimension for sequence length\n",
    "\t\t\tmodel.train()\n",
    "\t\t\t\n",
    "\t\t\toutputs = model(X_batch, X_batch[:, 0])\n",
    "\t\t\toutputs = (outputs[:, 0] + outputs[:, 1] ) / 2\n",
    "\t\t\t# outputs = outputs[:, 0]\n",
    "\t\t\ty = y_batch[:, 0]\n",
    "\t\t\t\n",
    "\t\t\t# Calculate losses\n",
    "\t\t\thub_loss = huber_loss(outputs, y, 0.02)\n",
    "\t\t\tmse_loss = F.mse_loss(outputs, y)\n",
    "\t\t\tmae_loss = F.l1_loss(outputs, y)\n",
    "\t\t\tmask = y >= 1e-10\n",
    "\t\t\ty_m = y[mask]\n",
    "\t\t\trelative_errors = torch.abs(outputs[mask] - y_m ) / y_m\n",
    "\t\t\tmre_loss = relative_errors.mean()\n",
    "\t\t\t\n",
    "\t\t\t# Backpropagation\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\thub_loss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\t\n",
    "\t\t\t# Accumulate losses\n",
    "\t\t\tepoch_mse += mse_loss.item()\n",
    "\t\t\tepoch_mae += mae_loss.item()\n",
    "\t\t\tepoch_mre += mre_loss.item()\n",
    "\t\t\tepoch_huber += hub_loss\n",
    "\t\t\n",
    "\t\tprint(f\"[ {datetime.now().strftime(\"%H:%M:%S\")} ] ----- Epoch [{epoch+1}/{num_epochs}] {'-'*150}\")\n",
    "\n",
    "\t\tavg_epoch_huber = epoch_huber / len(train_loader)\n",
    "\t\tavg_epoch_mse = epoch_mse / len(train_loader)\n",
    "\t\tavg_epoch_mae = epoch_mae / len(train_loader)\n",
    "\t\tavg_epoch_mre = epoch_mre / len(train_loader)\n",
    "\t\t\n",
    "\t\tprint(f\"{model.name:<50} | Huber loss: {avg_epoch_huber:<25} | MSE: {avg_epoch_mse:<25} | MAE: {avg_epoch_mae:<25} | MRE: {avg_epoch_mre:<25} |\")\n",
    "\t\t\n",
    "\t\tscheduler.step(avg_epoch_huber)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions import Normal\n",
    "\n",
    "def d1(K, T, sigma, F):\n",
    "\treturn (torch.log(F / K) + (0.5 * sigma**2) * T) / (sigma * torch.sqrt(T))\n",
    "\n",
    "def d2(d1, T, sigma):\n",
    "\treturn d1 - sigma * torch.sqrt(T)\n",
    "\n",
    "def delta(d1, F=1, option_type='call'):\n",
    "\tnormal_dist = Normal(0, 1)\n",
    "\n",
    "\tif option_type == 'call':\n",
    "\t\treturn normal_dist.cdf(d1)\n",
    "\telif option_type == 'put':\n",
    "\t\treturn normal_dist.cdf(d1) - 1\n",
    "\telse:\n",
    "\t\traise ValueError(\"Option type must be 'call' or 'put'\")\n",
    "\n",
    "def gamma(T, sigma, d1, F=1):\n",
    "\tnormal_dist = Normal(0, 1)\n",
    "\tpdf_d1 = torch.exp(normal_dist.log_prob(d1)) \n",
    "\n",
    "\treturn pdf_d1 / (F * sigma * torch.sqrt(T))\n",
    "\n",
    "def theta(K, T, sigma, d1, d2, F=1, r=0, option_type='call'):\n",
    "\tnormal_dist = Normal(0, 1)\n",
    "\tpdf_d1 = torch.exp(normal_dist.log_prob(d1)) \n",
    "\n",
    "\tif option_type == 'call':\n",
    "\t\treturn (-F * pdf_d1 * sigma / (2 * torch.sqrt(T)) - r * K * torch.exp(-r * T) * normal_dist.cdf(d2))\n",
    "\telif option_type == 'put':\n",
    "\t\treturn (-F * pdf_d1 * sigma / (2 * torch.sqrt(T)) + r * K * torch.exp(-r * T) * normal_dist.cdf(-d2))\n",
    "\telse:\n",
    "\t\traise ValueError(\"Option type must be 'call' or 'put'\")\n",
    "\n",
    "def vega(T, d1, F=1):\n",
    "\tnormal_dist = Normal(0, 1)\n",
    "\tpdf_d1 = torch.exp(normal_dist.log_prob(d1))\n",
    "\t\n",
    "\treturn F * pdf_d1 * torch.sqrt(T)\n",
    "\n",
    "def greeks(K, T, sigma, F=1, r=0, option_type='call'):\n",
    "\tdp = d1(K, T, sigma, F)\n",
    "\tdm = d2(dp, T, sigma)\n",
    "\t\n",
    "\treturn delta(dp, F, option_type), gamma(T, sigma, dp, F), theta(K, T, sigma, dp, dm, F, r, option_type), vega(T, dp, F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stage in range(num_stages):\n",
    "\tprint(f\"[ {datetime.now().strftime('%H:%M:%S')} ] ***** Stage [{stage+1}/{num_stages}] {'*'*150}\")\n",
    "\t\n",
    "\tfor epoch in range(num_epochs):\n",
    "\t\tcnt = 0\n",
    "\t\tepoch_huber = 0.\n",
    "\t\tepoch_mae = 0.\n",
    "\t\tepoch_mre = 0.\n",
    "\t\tepoch_mse = 0.\n",
    "\t\tepoch_delta_loss = 0.\n",
    "\t\tepoch_gamma_loss = 0.\n",
    "\t\t\n",
    "\t\tfor X_batch, y_batch in train_loader:\n",
    "\t\t\tX_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "\t\t\tX_batch = X_batch.clone().detach().requires_grad_(True)#torch.tensor(X_batch, dtype=torch.float64, requires_grad=True)\n",
    "\t\t\ty_batch = y_batch.clone().detach().requires_grad_(True)#torch.tensor(y_batch, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "\t\t\tmodel.train()\n",
    "\t\t\t\n",
    "\t\t\toutputs = model(X_batch, X_batch[:, 0])\n",
    "\t\t\toutputs = (outputs[:, 0] + outputs[:, 1]) / 2\n",
    "\t\t\ty = y_batch[:, 0]\n",
    "\t\t\t\n",
    "\t\t\t# Вычисление основных потерь\n",
    "\t\t\thub_loss = huber_loss(outputs, y, 0.02)\n",
    "\t\t\tmse_loss = F.mse_loss(outputs, y)\n",
    "\t\t\tmae_loss = F.l1_loss(outputs, y)\n",
    "\t\t\tmask = y >= 1e-10\n",
    "\t\t\ty_m = y[mask]\n",
    "\t\t\trelative_errors = torch.abs(outputs[mask] - y_m) / y_m\n",
    "\t\t\tmre_loss = relative_errors.mean()\n",
    "\t\t\t\n",
    "\t\t\t# Обнуление градиентов перед вычислением грека\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t\t# --- Вычисление дельты и гаммы ---\n",
    "\t\t\tdeltas, gammas, thetas, vegas = greeks(X_batch[:, 0], X_batch[:, 1], X_batch[:, 3] / torch.sqrt(X_batch[:, 1]))\n",
    "\n",
    "\t\t\toutputs.backward(torch.ones_like(outputs), retain_graph=True)\n",
    "\t\t\tK_grad = X_batch.grad[:, 0].clone()  \n",
    "\n",
    "\t\t\tX_batch.grad.zero_()\n",
    "\n",
    "\t\t\ty = model(X_batch, X_batch[:, 0])\n",
    "\t\t\ty = (y[:, 0] + y[:, 1] ) / 2\n",
    "\t\t\t# y = y[:, 0]\n",
    "\t\t\ty.backward(torch.ones_like(y), retain_graph=True)\n",
    "\t\t\tdelta_grad = X_batch.grad[:, 0].clone().requires_grad_(True)\n",
    "\n",
    "\t\t\tX_batch.grad.zero_()\n",
    "\n",
    "\t\t\t# Вычисление второго градиента (гамма)\n",
    "\t\t\ty = model(X_batch, X_batch[:, 0])\n",
    "\t\t\ty = (y[:, 0] + y[:, 1] ) / 2\n",
    "\t\t\t# y = y[:, 0]\n",
    "\t\t\ty.backward(torch.ones_like(y), retain_graph=True)\n",
    "\t\t\tdelta_grad.backward(torch.ones_like(delta_grad), retain_graph=True)\n",
    "\t\t\tgamma_grad = X_tensor.grad[:, 0].clone()\n",
    "\n",
    "\t\t\t# --- Вычисление потерь по дельте ---\n",
    "\t\t\toutputs.backward(torch.ones_like(outputs), retain_graph=True)  # Рассчитываем дельту\n",
    "\t\t\tdelta_pred = X_batch.grad[:, 0].clone().requires_grad_(True)  # Градиент по K — это дельта\n",
    "\t\t\tdelta_loss = F.mse_loss(delta_pred, deltas)\n",
    "\n",
    "\t\t\t# Обнуление градиентов для следующего шага\n",
    "\t\t\tX_batch.grad.zero_()\n",
    "\n",
    "\t\t\toutputs = model(X_batch, X_batch[:, 0])\n",
    "\t\t\toutputs = (outputs[:, 0] + outputs[:, 1]) / 2\n",
    "\n",
    "\t\t\t# --- Вычисление потерь по гамме ---\n",
    "\t\t\toutputs.backward(torch.ones_like(delta_pred), retain_graph=True)  # Рассчитываем гамму\n",
    "\t\t\tdelta_pred.backward(torch.ones_like(delta_pred), retain_graph=True)\n",
    "\t\t\tgamma_pred = X_batch.grad[:, 0].clone().detach()  # Градиент дельты — это гамма\n",
    "\t\t\tgamma_loss = F.mse_loss(gamma_pred, gammas)\n",
    "\n",
    "\t\t\t# Назад по градиенту для обновления параметров\n",
    "\t\t\tdelta_loss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\t\n",
    "\t\t\t# Аккумулируем потери для статистики\n",
    "\t\t\tepoch_mse += mse_loss.item()\n",
    "\t\t\tepoch_mae += mae_loss.item()\n",
    "\t\t\tepoch_mre += mre_loss.item()\n",
    "\t\t\tepoch_huber += hub_loss.item()\n",
    "\t\t\tepoch_delta_loss += delta_loss.item()\n",
    "\t\t\tepoch_gamma_loss += gamma_loss.item()\n",
    "\n",
    "\t\t# Логируем результаты для текущей эпохи\n",
    "\t\tprint(f\"[ {datetime.now().strftime('%H:%M:%S')} ] ----- Epoch [{epoch+1}/{num_epochs}] {'-'*150}\")\n",
    "\t\tavg_epoch_huber = epoch_huber / len(train_loader)\n",
    "\t\tavg_epoch_mse = epoch_mse / len(train_loader)\n",
    "\t\tavg_epoch_mae = epoch_mae / len(train_loader)\n",
    "\t\tavg_epoch_mre = epoch_mre / len(train_loader)\n",
    "\t\tavg_delta_loss = epoch_delta_loss / len(train_loader)\n",
    "\t\tavg_gamma_loss = epoch_gamma_loss / len(train_loader)\n",
    "\n",
    "\t\tprint(f\"{model.name:<25} | MSE: {avg_epoch_mse:<25} | MAE: {avg_epoch_mae:<25} | MRE: {avg_epoch_mre:<25} | Delta Loss: {avg_delta_loss:<25} | Gamma Loss: {avg_gamma_loss:<25} |\")\n",
    "\t\t\n",
    "\t\tscheduler.step(avg_delta_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stage in range(num_stages):\n",
    "\tprint(f\"[ {datetime.now().strftime('%H:%M:%S')} ] ***** Stage [{stage+1}/{num_stages}] {'*'*150}\")\n",
    "\t\n",
    "\tfor epoch in range(num_epochs):\n",
    "\t\tepoch_huber, epoch_mae, epoch_mre = 0., 0., 0.\n",
    "\t\tepoch_mse, epoch_delta_loss, epoch_gamma_loss = 0., 0., 0.\n",
    "\t\t\n",
    "\t\tfor X_batch, y_batch in train_loader:\n",
    "\t\t\tX_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t# Включение градиентов для X_batch\n",
    "\t\t\tX_batch = X_batch.clone().detach().requires_grad_(True)\n",
    "\t\t\ty_batch = y_batch.clone().detach().requires_grad_(True)\n",
    "\n",
    "\t\t\tmodel.train()\n",
    "\t\t\toutputs = model(X_batch, X_batch[:, 0])\n",
    "\t\t\toutputs = (outputs[:, 0] + outputs[:, 1]) / 2\n",
    "\t\t\ty = y_batch[:, 0]\n",
    "\t\t\t\n",
    "\t\t\t# Основные потери\n",
    "\t\t\thub_loss = huber_loss(outputs, y, 0.02)\n",
    "\t\t\tmse_loss = F.mse_loss(outputs, y)\n",
    "\t\t\tmae_loss = F.l1_loss(outputs, y)\n",
    "\t\t\t\n",
    "\t\t\t# Вычисление относительных ошибок\n",
    "\t\t\tmask = y >= 1e-10\n",
    "\t\t\ty_m = y[mask]\n",
    "\t\t\trelative_errors = torch.abs(outputs[mask] - y_m) / y_m\n",
    "\t\t\tmre_loss = relative_errors.mean()\n",
    "\n",
    "\t\t\t# --- Вычисление дельты ---\n",
    "\t\t\tdeltas, gammas, thetas, vegas = greeks(X_batch[:, 0], X_batch[:, 1], X_batch[:, 3] / torch.sqrt(X_batch[:, 1]))\n",
    "\n",
    "\t\t\t# Рассчитываем градиент по страйку (дельту)\n",
    "\t\t\toutputs.backward(torch.ones_like(outputs), retain_graph=True)\n",
    "\t\t\tdelta_grad = X_batch.grad[:, 0].clone().requires_grad_(True)  # Сохраняем градиенты\n",
    "\n",
    "\t\t\t# # Рассчитываем второй градиент (гамму)\n",
    "\t\t\t# delta_grad.backward(torch.ones_like(delta_grad), retain_graph=True)\n",
    "\t\t\t# gamma_grad = X_batch.grad[:, 0].clone().detach()\n",
    "\n",
    "\t\t\t# Вычисление потерь по дельте и гамме\n",
    "\t\t\tdelta_loss = F.mse_loss(delta_grad, deltas)\n",
    "\t\t\t# gamma_loss = F.mse_loss(gamma_grad, gammas)\n",
    "\n",
    "\t\t\t# Назад по градиенту для обновления параметров\n",
    "\t\t\toptimizer.zero_grad()  # Обнуление градиентов перед шагом\n",
    "\t\t\tdelta_loss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\n",
    "\t\t\t# Аккумулируем потери для статистики\n",
    "\t\t\tepoch_huber += hub_loss.item()\n",
    "\t\t\tepoch_mse += mse_loss.item()\n",
    "\t\t\tepoch_mae += mae_loss.item()\n",
    "\t\t\tepoch_mre += mre_loss.item()\n",
    "\t\t\tepoch_delta_loss += delta_loss.item()\n",
    "\t\t\t# epoch_gamma_loss += gamma_loss.item()\n",
    "\n",
    "\t\t# Логирование\n",
    "\t\tavg_epoch_huber = epoch_huber / len(train_loader)\n",
    "\t\tavg_epoch_mse = epoch_mse / len(train_loader)\n",
    "\t\tavg_epoch_mae = epoch_mae / len(train_loader)\n",
    "\t\tavg_epoch_mre = epoch_mre / len(train_loader)\n",
    "\t\tavg_delta_loss = epoch_delta_loss / len(train_loader)\n",
    "\t\tavg_gamma_loss = 0#epoch_gamma_loss / len(train_loader)\n",
    "\n",
    "\t\tprint(f\"[ {datetime.now().strftime('%H:%M:%S')} ] ----- Epoch [{epoch+1}/{num_epochs}] {'-'*150}\")\n",
    "\t\tprint(f\"{model.name:<25} | MSE: {avg_epoch_mse:<25} | MAE: {avg_epoch_mae:<25} | MRE: {avg_epoch_mre:<25} | Delta Loss: {avg_delta_loss:<25} | Gamma Loss: {avg_gamma_loss:<25} |\")\n",
    "\t\t\n",
    "\t\tscheduler.step(avg_delta_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-cosh loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 18:51:10 ] ***** Stage [1/1] ******************************************************************************************************************************************************\n",
      "[ 18:51:30 ] ----- Epoch [1/30] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Black Model                                        | Huber loss: 2.805671036771152e-05     | MSE: 0.01731447072122037       | MAE: 0.089106594409645         | MRE: 3058633.5212305137        |\n",
      "[ 18:51:50 ] ----- Epoch [2/30] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Black Model                                        | Huber loss: 2.805671036771152e-05     | MSE: 0.0010450003118724588     | MAE: 0.023971658696214888      | MRE: 771303.588995485          |\n",
      "[ 18:52:09 ] ----- Epoch [3/30] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Black Model                                        | Huber loss: 2.805671036771152e-05     | MSE: 0.0001512994754532175     | MAE: 0.00895387339441951       | MRE: 210626.51115082315        |\n",
      "[ 18:52:27 ] ----- Epoch [4/30] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Black Model                                        | Huber loss: 2.805671036771152e-05     | MSE: 6.892608815181244e-05     | MAE: 0.005883380302780205      | MRE: 111147.05435583208        |\n",
      "[ 18:52:45 ] ----- Epoch [5/30] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Black Model                                        | Huber loss: 2.805671036771152e-05     | MSE: 4.9353556588203586e-05    | MAE: 0.004968252232131193      | MRE: 89373.80513092178         |\n",
      "[ 18:53:02 ] ----- Epoch [6/30] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Black Model                                        | Huber loss: 2.805671036771152e-05     | MSE: 3.903264970802214e-05     | MAE: 0.004442454011402691      | MRE: 77508.10814262812         |\n",
      "[ 18:53:19 ] ----- Epoch [7/30] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Black Model                                        | Huber loss: 2.805671036771152e-05     | MSE: 3.368304325566162e-05     | MAE: 0.0041201268446925815     | MRE: 69080.51673518943         |\n",
      "[ 18:53:36 ] ----- Epoch [8/30] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Black Model                                        | Huber loss: 2.805671036771152e-05     | MSE: 2.956845673858399e-05     | MAE: 0.00385377370228547       | MRE: 63424.132843474166        |\n",
      "[ 18:53:53 ] ----- Epoch [9/30] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Black Model                                        | Huber loss: 2.805671036771152e-05     | MSE: 2.7425744214238394e-05    | MAE: 0.0036998401798948245     | MRE: 58765.5387860732          |\n",
      "[ 18:54:10 ] ----- Epoch [10/30] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Black Model                                        | Huber loss: 2.805671036771152e-05     | MSE: 2.4939494013544563e-05    | MAE: 0.003519000832304912      | MRE: 53088.027463344246        |\n",
      "[ 18:54:27 ] ----- Epoch [11/30] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Black Model                                        | Huber loss: 2.805671036771152e-05     | MSE: 2.3431339014483393e-05    | MAE: 0.0033873751352765423     | MRE: 49596.209813927715        |\n",
      "[ 18:54:44 ] ----- Epoch [12/30] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Black Model                                        | Huber loss: 2.805671036771152e-05     | MSE: 2.2073221655830494e-05    | MAE: 0.0032872964734381215     | MRE: 45795.7671171294          |\n",
      "[ 18:55:01 ] ----- Epoch [13/30] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Black Model                                        | Huber loss: 2.805671036771152e-05     | MSE: 2.084050045904518e-05     | MAE: 0.0031834157982392804     | MRE: 44370.02082890216         |\n",
      "[ 18:55:18 ] ----- Epoch [14/30] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Black Model                                        | Huber loss: 2.805671036771152e-05     | MSE: 1.9599185384201923e-05    | MAE: 0.003092811103534611      | MRE: 40402.83255294311         |\n",
      "[ 18:55:35 ] ----- Epoch [15/30] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Black Model                                        | Huber loss: 2.805671036771152e-05     | MSE: 1.942398345676197e-05     | MAE: 0.003049764804336584      | MRE: 38675.91596824273         |\n",
      "[ 18:55:52 ] ----- Epoch [16/30] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Black Model                                        | Huber loss: 2.805671036771152e-05     | MSE: 1.8326963362392304e-05    | MAE: 0.0029648561416231053     | MRE: 37599.43025517452         |\n",
      "[ 18:56:09 ] ----- Epoch [17/30] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Black Model                                        | Huber loss: 2.805671036771152e-05     | MSE: 1.7815382839989627e-05    | MAE: 0.002926117903623292      | MRE: 36467.50197642514         |\n",
      "[ 18:56:27 ] ----- Epoch [18/30] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Black Model                                        | Huber loss: 2.805671036771152e-05     | MSE: 1.7085708220998428e-05    | MAE: 0.0028566735476514334     | MRE: 34857.12117580216         |\n",
      "[ 18:56:47 ] ----- Epoch [19/30] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Black Model                                        | Huber loss: 2.805671036771152e-05     | MSE: 1.664699292385959e-05     | MAE: 0.0028183495533419126     | MRE: 33416.366910919016        |\n",
      "[ 18:57:06 ] ----- Epoch [20/30] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Black Model                                        | Huber loss: 2.805671036771152e-05     | MSE: 1.617160993766033e-05     | MAE: 0.0027698345718942087     | MRE: 32453.09566223582         |\n",
      "[ 18:57:24 ] ----- Epoch [21/30] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Black Model                                        | Huber loss: 2.805671036771152e-05     | MSE: 1.568822873353176e-05     | MAE: 0.002725351934926964      | MRE: 30692.10454301053         |\n",
      "[ 18:57:42 ] ----- Epoch [22/30] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Black Model                                        | Huber loss: 2.805671036771152e-05     | MSE: 1.541930846803333e-05     | MAE: 0.002688442294558928      | MRE: 29721.88937755512         |\n",
      "[ 18:58:00 ] ----- Epoch [23/30] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Black Model                                        | Huber loss: 2.805671036771152e-05     | MSE: 1.523920125001209e-05     | MAE: 0.0026733518137175855     | MRE: 29054.824303924455        |\n",
      "[ 18:58:18 ] ----- Epoch [24/30] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Black Model                                        | Huber loss: 2.805671036771152e-05     | MSE: 1.4769747799491174e-05    | MAE: 0.0026342328758102098     | MRE: 28172.53923078817         |\n",
      "[ 18:58:35 ] ----- Epoch [25/30] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Black Model                                        | Huber loss: 2.805671036771152e-05     | MSE: 1.4599723825824199e-05    | MAE: 0.002613716768853416      | MRE: 27587.680738012714        |\n",
      "[ 18:58:53 ] ----- Epoch [26/30] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Black Model                                        | Huber loss: 2.805671036771152e-05     | MSE: 1.4126480054870121e-05    | MAE: 0.002568947940246465      | MRE: 26875.012025421187        |\n",
      "[ 18:59:10 ] ----- Epoch [27/30] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Black Model                                        | Huber loss: 2.805671036771152e-05     | MSE: 1.3942346808017669e-05    | MAE: 0.0025501496988805466     | MRE: 25503.138978476985        |\n",
      "[ 18:59:28 ] ----- Epoch [28/30] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Black Model                                        | Huber loss: 2.805671036771152e-05     | MSE: 1.3692698933634936e-05    | MAE: 0.0025199557347813984     | MRE: 25726.191536663966        |\n",
      "[ 18:59:46 ] ----- Epoch [29/30] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Black Model                                        | Huber loss: 2.805671036771152e-05     | MSE: 1.3380714134394212e-05    | MAE: 0.0024847053966230237     | MRE: 24492.420727477875        |\n",
      "[ 19:00:03 ] ----- Epoch [30/30] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Black Model                                        | Huber loss: 2.805671036771152e-05     | MSE: 1.3421092845980374e-05    | MAE: 0.0024886848291776017     | MRE: 23859.429674730753        |\n"
     ]
    }
   ],
   "source": [
    "for stage in range(num_stages):\n",
    "\tprint(f\"[ {datetime.now().strftime(\"%H:%M:%S\")} ] ***** Stage [{stage+1}/{num_stages}] {'*'*150}\")\n",
    "\t\n",
    "\tfor epoch in range(num_epochs):\n",
    "\t\tcnt = 0\n",
    "\t\tepoch_lcosh = 0.\n",
    "\t\tepoch_mae = 0.\n",
    "\t\tepoch_mre = 0.\n",
    "\t\tepoch_mse = 0.\n",
    "\t\t\n",
    "\t\tfor X_batch, y_batch in train_loader:\n",
    "\t\t\tX_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "\t\t\tmodel.train()\n",
    "\t\t\t\n",
    "\t\t\toutputs = model(X_batch, X_batch[:, 0])\n",
    "\t\t\toutputs = (outputs[:, 0] + outputs[:, 1] ) / 2\n",
    "\t\t\ty = y_batch[:, 0]\n",
    "\t\t\t\n",
    "\t\t\t# Calculate losses\n",
    "\t\t\tlcosh_loss = torch.mean(torch.log(torch.cosh(outputs-y)))\n",
    "\t\t\tmse_loss = F.mse_loss(outputs, y)\n",
    "\t\t\tmae_loss = F.l1_loss(outputs, y)\n",
    "\t\t\tmask = y >= 1e-10\n",
    "\t\t\ty_m = y[mask]\n",
    "\t\t\trelative_errors = torch.abs(outputs[mask] - y_m ) / y_m\n",
    "\t\t\tmre_loss = relative_errors.sum()\n",
    "\n",
    "\t\t\tcnt += len(relative_errors)\n",
    "\t\t\t\n",
    "\t\t\t# Backpropagation\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\tlcosh_loss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\t\n",
    "\t\t\t# Accumulate losses\n",
    "\t\t\tepoch_mse += mse_loss.item()\n",
    "\t\t\tepoch_mae += mae_loss.item()\n",
    "\t\t\tepoch_mre += mre_loss.item()\n",
    "\t\t\tepoch_lcosh += hub_loss\n",
    "\t\t\n",
    "\t\tprint(f\"[ {datetime.now().strftime(\"%H:%M:%S\")} ] ----- Epoch [{epoch+1}/{num_epochs}] {'-'*150}\")\n",
    "\n",
    "\t\tavg_epoch_lcosh = epoch_lcosh / len(train_loader)\n",
    "\t\tavg_epoch_mse = epoch_mse / len(train_loader)\n",
    "\t\tavg_epoch_mae = epoch_mae / len(train_loader)\n",
    "\t\tavg_epoch_mre = epoch_mre / cnt\n",
    "\t\t\n",
    "\t\tprint(f\"{model.name:<50} | Huber loss: {avg_epoch_lcosh:<25} | MSE: {avg_epoch_mse:<25} | MAE: {avg_epoch_mae:<25} | MRE: {avg_epoch_mre:<25} |\")\n",
    "\t\t\n",
    "\t\tscheduler.step(avg_epoch_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(num_epochs):\n",
    "# \t# Initialize epoch metrics for each model\n",
    "# \tepoch_mae = [0.] * len(models)\n",
    "# \tepoch_mre = [0.] * len(models)\n",
    "# \tepoch_mse = [0.] * len(models)\n",
    "\t\n",
    "# \tfor X_batch, y_batch in train_loader:\n",
    "# \t\tX_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\t\t\n",
    "# \t\t# Loop over each model\n",
    "# \t\tfor i, model in enumerate(models):\n",
    "# \t\t\tmodel.train()\n",
    "\t\t\t\n",
    "# \t\t\t# Forward pass\n",
    "# \t\t\toutputs = model(X_batch, X_batch[:, 0])\n",
    "# \t\t\toutputs = (outputs[:, 0] + outputs[:, 1] ) / 2\n",
    "# \t\t\ty = y_batch[:, 0]\n",
    "\t\t\t\n",
    "# \t\t\t# Calculate losses\n",
    "# \t\t\tmse_loss = F.mse_loss(outputs, y)\n",
    "# \t\t\tmae_loss = F.l1_loss(outputs, y)\n",
    "# \t\t\trelative_errors = torch.abs(outputs - y) / (y + 1e-8)\n",
    "# \t\t\tmre_loss = relative_errors.mean()\n",
    "\t\t\t\n",
    "# \t\t\t# Backpropagation\n",
    "# \t\t\toptimizers.zero_grad()\n",
    "# \t\t\tmse_loss.backward()\n",
    "# \t\t\toptimizers.step()\n",
    "\t\t\t\n",
    "# \t\t\t# Accumulate losses for this model\n",
    "# \t\t\tepoch_mse += mse_loss.item()\n",
    "# \t\t\tepoch_mae += mae_loss.item()\n",
    "# \t\t\tepoch_mre += mre_loss.item()\n",
    "\t\n",
    "# \tprint(f\"[ {datetime.now().strftime(\"%H:%M:%S\")} ] ----- Epoch [{epoch+1}/{num_epochs}] {'-'*150}\")\n",
    "# \t# Average metrics for each model\n",
    "# \tfor i in range(len(models)):\n",
    "# \t\tavg_epoch_mse = epoch_mse / len(train_loader)\n",
    "# \t\tavg_epoch_mae = epoch_mae / len(train_loader)\n",
    "# \t\tavg_epoch_mre = epoch_mre / len(train_loader)\n",
    "\t\t\n",
    "# \t\tprint(f\"{models.name:<50} | MSE: {avg_epoch_mse:<25} | MAE: {avg_epoch_mae:<25} | MRE: {avg_epoch_mre:<25} |\")\n",
    "\t\t\n",
    "# \t\t# Scheduler step\n",
    "# \t\tschedulers.step(avg_epoch_mse)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 19:38:53 ] ***** Stage [1/1] ******************************************************************************************************************************************************\n",
      "[ 19:39:15 ] ----- Epoch [1/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 0.0012387422443283591     | MAE: 0.016218780921354994      | MRE: 418882.8272023318         |\n",
      "[ 19:39:36 ] ----- Epoch [2/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 0.00014922346122831482    | MAE: 0.008199835669306535      | MRE: 207551.09525587354        |\n",
      "[ 19:39:56 ] ----- Epoch [3/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 9.652108094338008e-05     | MAE: 0.006528006532513435      | MRE: 168242.51103001524        |\n",
      "[ 19:40:16 ] ----- Epoch [4/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 6.87586467265643e-05      | MAE: 0.005471410345171871      | MRE: 145380.22796220047        |\n",
      "[ 19:40:35 ] ----- Epoch [5/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.079765233058875e-05     | MAE: 0.004623114272377649      | MRE: 122994.51527700896        |\n",
      "[ 19:40:55 ] ----- Epoch [6/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 3.9394520332995863e-05    | MAE: 0.00408765775339114       | MRE: 109148.44934601455        |\n",
      "[ 19:41:15 ] ----- Epoch [7/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 3.287629039280334e-05     | MAE: 0.003723022823488402      | MRE: 97562.06930063842         |\n",
      "[ 19:41:35 ] ----- Epoch [8/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 2.6724876076194554e-05    | MAE: 0.003390223252122783      | MRE: 83331.05340331029         |\n",
      "[ 19:41:54 ] ----- Epoch [9/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 2.6230625586535602e-05    | MAE: 0.0032113390023545845     | MRE: 76096.97057485204         |\n",
      "[ 19:42:15 ] ----- Epoch [10/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 2.3073379388053323e-05    | MAE: 0.0030000235985624952     | MRE: 68721.48932558885         |\n",
      "[ 19:42:34 ] ----- Epoch [11/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 1.8987465590463745e-05    | MAE: 0.0028084888077078434     | MRE: 61449.96835995842         |\n",
      "[ 19:42:54 ] ----- Epoch [12/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 1.901935439557273e-05     | MAE: 0.002745105216441431      | MRE: 58002.04326503019         |\n",
      "[ 19:43:15 ] ----- Epoch [13/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 2.0449666910887182e-05    | MAE: 0.002736081077938802      | MRE: 53761.631743474805        |\n",
      "[ 19:43:36 ] ----- Epoch [14/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 1.8789656196962676e-05    | MAE: 0.0025505764211869973     | MRE: 48431.019596067956        |\n",
      "[ 19:43:56 ] ----- Epoch [15/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 1.7259932315619904e-05    | MAE: 0.002548279647426388      | MRE: 48026.58565331684         |\n",
      "[ 19:44:17 ] ----- Epoch [16/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 1.4876287637763628e-05    | MAE: 0.002458272759162793      | MRE: 45976.22413623035         |\n",
      "[ 19:44:37 ] ----- Epoch [17/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 1.559505264290456e-05     | MAE: 0.002416871523997096      | MRE: 44076.596934364105        |\n",
      "[ 19:44:58 ] ----- Epoch [18/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 1.7086039121758003e-05    | MAE: 0.002422793866691814      | MRE: 44904.582390032025        |\n",
      "[ 19:45:18 ] ----- Epoch [19/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 1.3859318921267911e-05    | MAE: 0.0023265710456272168     | MRE: 42184.245911390855        |\n",
      "[ 19:45:39 ] ----- Epoch [20/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 1.5320272697351224e-05    | MAE: 0.0022930684112135824     | MRE: 43340.67378069819         |\n",
      "[ 19:46:00 ] ----- Epoch [21/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 1.3768079225490903e-05    | MAE: 0.0022669791733738163     | MRE: 42621.01628724152         |\n",
      "[ 19:46:20 ] ----- Epoch [22/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 1.4179555519541103e-05    | MAE: 0.002267303399995639      | MRE: 41500.11583504625         |\n",
      "[ 19:46:41 ] ----- Epoch [23/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 1.1579474002648462e-05    | MAE: 0.002158873025545145      | MRE: 40856.11200877754         |\n",
      "[ 19:47:01 ] ----- Epoch [24/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 1.405316922259296e-05     | MAE: 0.002209999212515539      | MRE: 41234.74652027598         |\n",
      "[ 19:47:22 ] ----- Epoch [25/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 1.2898148274089571e-05    | MAE: 0.0021270317295277582     | MRE: 39655.72085204569         |\n",
      "[ 19:47:43 ] ----- Epoch [26/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 2.1545236183895844e-05    | MAE: 0.0022622939456764233     | MRE: 45368.09537404676         |\n",
      "[ 19:48:03 ] ----- Epoch [27/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 1.0553772309346885e-05    | MAE: 0.002055513247180771      | MRE: 36505.55786115647         |\n",
      "[ 19:48:24 ] ----- Epoch [28/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 1.2198189445486771e-05    | MAE: 0.002116263327088409      | MRE: 38238.74791462189         |\n",
      "[ 19:48:45 ] ----- Epoch [29/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 9.780553133749212e-06     | MAE: 0.002024500715369133      | MRE: 37074.03761893683         |\n",
      "[ 19:49:05 ] ----- Epoch [30/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 1.3182042901637013e-05    | MAE: 0.002089894418080908      | MRE: 36240.763912565424        |\n",
      "[ 19:49:25 ] ----- Epoch [31/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 1.1106110270185981e-05    | MAE: 0.0020116039954258982     | MRE: 37061.81122309347         |\n",
      "[ 19:49:45 ] ----- Epoch [32/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 1.2513205633517374e-05    | MAE: 0.0020596209273744484     | MRE: 37974.20377120092         |\n",
      "[ 19:50:05 ] ----- Epoch [33/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 1.1457852585171776e-05    | MAE: 0.0020205901905387545     | MRE: 36702.434092707415        |\n",
      "[ 19:50:25 ] ----- Epoch [34/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 1.1389475510667158e-05    | MAE: 0.0019921247158246726     | MRE: 35659.902655264756        |\n",
      "[ 19:50:46 ] ----- Epoch [35/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 1.2630456007398428e-05    | MAE: 0.001982886104142974      | MRE: 35664.478294668545        |\n",
      "[ 19:51:07 ] ----- Epoch [36/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 1.3358532211332986e-05    | MAE: 0.0019869029688083717     | MRE: 37753.56936554856         |\n",
      "[ 19:51:26 ] ----- Epoch [37/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 1.2252543690753843e-05    | MAE: 0.001994120992982147      | MRE: 36003.745758809244        |\n",
      "[ 19:51:46 ] ----- Epoch [38/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 1.647638961731367e-05     | MAE: 0.0020311464226926996     | MRE: 35049.31006601917         |\n",
      "[ 19:52:06 ] ----- Epoch [39/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 1.789747751577922e-05     | MAE: 0.0019931626354357562     | MRE: 35689.75413092107         |\n",
      "[ 19:52:25 ] ----- Epoch [40/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.9439376255731825e-06    | MAE: 0.0014773479236986499     | MRE: 15701.196099601468        |\n",
      "[ 19:52:45 ] ----- Epoch [41/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.951101377791705e-06     | MAE: 0.0014394696145296202     | MRE: 11557.096989021611        |\n",
      "[ 19:53:04 ] ----- Epoch [42/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.72108332067665e-06      | MAE: 0.00143376397550133       | MRE: 10423.84828329683         |\n",
      "[ 19:53:24 ] ----- Epoch [43/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.654311205905678e-06     | MAE: 0.0014211653913944065     | MRE: 10294.747220937246        |\n",
      "[ 19:53:45 ] ----- Epoch [44/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.72531580608321e-06      | MAE: 0.0014151020029118287     | MRE: 9413.651475507222         |\n",
      "[ 19:54:05 ] ----- Epoch [45/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.6886097957280995e-06    | MAE: 0.001413272519250934      | MRE: 9515.447081429838         |\n",
      "[ 19:54:25 ] ----- Epoch [46/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.609057405248755e-06     | MAE: 0.0014090600785911325     | MRE: 9416.561222529432         |\n",
      "[ 19:54:44 ] ----- Epoch [47/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.558684266372228e-06     | MAE: 0.0014079612638249331     | MRE: 9276.576342985652         |\n",
      "[ 19:55:04 ] ----- Epoch [48/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.455519176427783e-06     | MAE: 0.001398406823939995      | MRE: 9167.587867668708         |\n",
      "[ 19:55:25 ] ----- Epoch [49/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.469079065088315e-06     | MAE: 0.001392298695305406      | MRE: 9195.660062423303         |\n",
      "[ 19:55:47 ] ----- Epoch [50/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.4813016760766185e-06    | MAE: 0.0013891384601808078     | MRE: 8769.654184279167         |\n",
      "[ 19:56:09 ] ----- Epoch [51/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.464010040326405e-06     | MAE: 0.0013948086029311473     | MRE: 9180.4026619549           |\n",
      "[ 19:56:31 ] ----- Epoch [52/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.435918630515911e-06     | MAE: 0.0013904334913482494     | MRE: 9221.052631353416         |\n",
      "[ 19:56:53 ] ----- Epoch [53/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.508453566320397e-06     | MAE: 0.0013884599390805979     | MRE: 8666.601343225504         |\n",
      "[ 19:57:14 ] ----- Epoch [54/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.338838574703968e-06     | MAE: 0.0013779037383968577     | MRE: 8782.309863988625         |\n",
      "[ 19:57:34 ] ----- Epoch [55/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.328845983437046e-06     | MAE: 0.001382980484393319      | MRE: 8792.108163202787         |\n",
      "[ 19:57:54 ] ----- Epoch [56/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.394644755734472e-06     | MAE: 0.001387961529648681      | MRE: 9160.785604251496         |\n",
      "[ 19:58:14 ] ----- Epoch [57/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.3951316659966725e-06    | MAE: 0.0013792247907259525     | MRE: 8744.605774444317         |\n",
      "[ 19:58:37 ] ----- Epoch [58/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.354269315162367e-06     | MAE: 0.0013707923765938304     | MRE: 8630.125176877309         |\n",
      "[ 19:58:59 ] ----- Epoch [59/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.291186846798455e-06     | MAE: 0.0013689277296222888     | MRE: 8550.44209461262          |\n",
      "[ 19:59:19 ] ----- Epoch [60/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.335608153061823e-06     | MAE: 0.0013738908943904536     | MRE: 8530.328519407329         |\n",
      "[ 19:59:39 ] ----- Epoch [61/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.355927528779444e-06     | MAE: 0.0013767651036279028     | MRE: 8529.519531418253         |\n",
      "[ 19:59:59 ] ----- Epoch [62/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.25242002118665e-06      | MAE: 0.0013663545611435552     | MRE: 8630.801426725273         |\n",
      "[ 20:00:18 ] ----- Epoch [63/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.363495913084563e-06     | MAE: 0.0013706433717524706     | MRE: 8656.234068322183         |\n",
      "[ 20:00:39 ] ----- Epoch [64/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.278441380915262e-06     | MAE: 0.0013595910148904513     | MRE: 8318.348754407305         |\n",
      "[ 20:00:59 ] ----- Epoch [65/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.281614589673633e-06     | MAE: 0.0013608980958645153     | MRE: 8521.972715165624         |\n",
      "[ 20:01:19 ] ----- Epoch [66/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.246130087601658e-06     | MAE: 0.0013610492548606138     | MRE: 8511.030485459663         |\n",
      "[ 20:01:40 ] ----- Epoch [67/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.204212130601135e-06     | MAE: 0.0013537272172070534     | MRE: 8355.616476634854         |\n",
      "[ 20:02:00 ] ----- Epoch [68/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.1949919416163805e-06    | MAE: 0.0013581877032656524     | MRE: 8444.729660735917         |\n",
      "[ 20:02:19 ] ----- Epoch [69/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.21244303667477e-06      | MAE: 0.001355869275479649      | MRE: 8182.688342152088         |\n",
      "[ 20:02:40 ] ----- Epoch [70/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.148379978228155e-06     | MAE: 0.0013517910971145412     | MRE: 8429.636534140373         |\n",
      "[ 20:03:00 ] ----- Epoch [71/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.152982115844362e-06     | MAE: 0.0013454088681482447     | MRE: 8109.174462813303         |\n",
      "[ 20:03:19 ] ----- Epoch [72/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.118252723121766e-06     | MAE: 0.0013501882362650256     | MRE: 8396.771856234196         |\n",
      "[ 20:03:39 ] ----- Epoch [73/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.103692668340508e-06     | MAE: 0.001349963633528329      | MRE: 8500.614908571171         |\n",
      "[ 20:04:00 ] ----- Epoch [74/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.100911011430704e-06     | MAE: 0.0013436118787094812     | MRE: 8257.23190742777          |\n",
      "[ 20:04:21 ] ----- Epoch [75/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.19617336222212e-06      | MAE: 0.0013492224946537667     | MRE: 8217.946390287576         |\n",
      "[ 20:04:42 ] ----- Epoch [76/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.15026240964716e-06      | MAE: 0.0013438924331132952     | MRE: 8135.548101201971         |\n",
      "[ 20:05:02 ] ----- Epoch [77/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.165475766393652e-06     | MAE: 0.0013437076680328163     | MRE: 8116.081660834531         |\n",
      "[ 20:05:22 ] ----- Epoch [78/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.159006911928947e-06     | MAE: 0.0013397749077444718     | MRE: 7966.585883014668         |\n",
      "[ 20:05:42 ] ----- Epoch [79/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.12041796988192e-06      | MAE: 0.0013391293860537479     | MRE: 8422.361072889185         |\n",
      "[ 20:06:03 ] ----- Epoch [80/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.081694120218604e-06     | MAE: 0.0013341546253655503     | MRE: 8240.245278989409         |\n",
      "[ 20:06:24 ] ----- Epoch [81/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.12165406578113e-06      | MAE: 0.0013373231701860485     | MRE: 8017.24967375762          |\n",
      "[ 20:06:44 ] ----- Epoch [82/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.071177270674405e-06     | MAE: 0.0013365385790445725     | MRE: 8381.411923136136         |\n",
      "[ 20:07:04 ] ----- Epoch [83/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.008354553198187e-06     | MAE: 0.001330499631317668      | MRE: 8145.020132433526         |\n",
      "[ 20:07:24 ] ----- Epoch [84/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.014333148909211e-06     | MAE: 0.0013314563759169598     | MRE: 8273.87250999321          |\n",
      "[ 20:07:44 ] ----- Epoch [85/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.009602411848705e-06     | MAE: 0.0013310634036770397     | MRE: 8152.344051372922         |\n",
      "[ 20:08:04 ] ----- Epoch [86/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.053064812235165e-06     | MAE: 0.0013293565355030205     | MRE: 8042.889234805228         |\n",
      "[ 20:56:42 ] ----- Epoch [87/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 4.95476633364074e-06      | MAE: 0.0013235970999578759     | MRE: 8219.948232812734         |\n",
      "[ 20:57:07 ] ----- Epoch [88/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.06430013441746e-06      | MAE: 0.0013268330946032047     | MRE: 7721.633987024481         |\n",
      "[ 20:57:29 ] ----- Epoch [89/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 4.92047770933667e-06      | MAE: 0.0013183847058673473     | MRE: 8278.52706668364          |\n",
      "[ 20:57:51 ] ----- Epoch [90/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 4.91360960441211e-06      | MAE: 0.0013180016904914776     | MRE: 8006.105497178814         |\n",
      "[ 20:58:11 ] ----- Epoch [91/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 4.971602756589971e-06     | MAE: 0.0013259061229859614     | MRE: 8149.407906943966         |\n",
      "[ 20:58:32 ] ----- Epoch [92/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 4.9350139282259205e-06    | MAE: 0.0013235309293711584     | MRE: 8300.68275612705          |\n",
      "[ 20:58:52 ] ----- Epoch [93/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 4.920711867657369e-06     | MAE: 0.0013237426256124266     | MRE: 7998.797558443173         |\n",
      "[ 20:59:13 ] ----- Epoch [94/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 5.027973301248225e-06     | MAE: 0.0013182285319965979     | MRE: 7894.557873713496         |\n",
      "[ 20:59:34 ] ----- Epoch [95/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 4.92248368956161e-06      | MAE: 0.0012837320837882351     | MRE: 5086.522334481092         |\n",
      "[ 20:59:54 ] ----- Epoch [96/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 4.933197045190606e-06     | MAE: 0.0012807706627980598     | MRE: 4827.4510381416985        |\n",
      "[ 21:00:13 ] ----- Epoch [97/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 4.864009468506319e-06     | MAE: 0.0012781653748213675     | MRE: 4757.928640430412         |\n",
      "[ 21:00:32 ] ----- Epoch [98/100] ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  | MSE: 4.878617180965962e-06     | MAE: 0.0012782972098539624     | MRE: 4804.628569736278         |\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[0;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 29\u001b[0m \u001b[43mmae_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Accumulate losses\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\serpo\\Documents\\Python\\Black\\.venv\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\serpo\\Documents\\Python\\Black\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\serpo\\Documents\\Python\\Black\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for stage in range(num_stages):\n",
    "\tprint(f\"[ {datetime.now().strftime(\"%H:%M:%S\")} ] ***** Stage [{stage+1}/{num_stages}] {'*'*150}\")\n",
    "\t\n",
    "\tfor epoch in range(num_epochs):\n",
    "\t\tepoch_mae = 0.\n",
    "\t\tepoch_mre = 0.\n",
    "\t\tepoch_mse = 0.\n",
    "\t\t\n",
    "\t\tfor X_batch, y_batch in train_loader:\n",
    "\t\t\tX_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "\t\t\tmodel.train()\n",
    "\t\t\t\n",
    "\t\t\toutputs = model(X_batch)\n",
    "\t\t\t# outputs = (outputs[:, 0] + outputs[:, 1] ) / 2\n",
    "\t\t\toutputs=outputs[:, 0]\n",
    "\t\t\ty = y_batch[:, 0]\n",
    "\t\t\t\n",
    "\t\t\t# Calculate losses\n",
    "\t\t\tmse_loss = F.mse_loss(outputs, y)\n",
    "\t\t\tmae_loss = F.l1_loss(outputs, y)\n",
    "\t\t\tmask = y >= 1e-10\n",
    "\t\t\ty_m = y[mask]\n",
    "\t\t\trelative_errors = torch.abs(outputs[mask] - y_m ) / y_m\n",
    "\t\t\tmre_loss = relative_errors.mean()\n",
    "\t\t\t\n",
    "\t\t\t# Backpropagation\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\t\t\tmae_loss.backward()\n",
    "\t\t\toptimizer.step()\n",
    "\t\t\t\n",
    "\t\t\t# Accumulate losses\n",
    "\t\t\tepoch_mse += mse_loss.item()\n",
    "\t\t\tepoch_mae += mae_loss.item()\n",
    "\t\t\tepoch_mre += mre_loss.item()\n",
    "\t\t\n",
    "\t\tprint(f\"[ {datetime.now().strftime(\"%H:%M:%S\")} ] ----- Epoch [{epoch+1}/{num_epochs}] {'-'*150}\")\n",
    "\n",
    "\t\tavg_epoch_mse = epoch_mse / len(train_loader)\n",
    "\t\tavg_epoch_mae = epoch_mae / len(train_loader)\n",
    "\t\tavg_epoch_mre = epoch_mre / len(train_loader)\n",
    "\t\t\n",
    "\t\tprint(f\"{model.name:<50} | MSE: {avg_epoch_mse:<25} | MAE: {avg_epoch_mae:<25} | MRE: {avg_epoch_mre:<25} |\")\n",
    "\t\t\n",
    "\t\tscheduler.step(avg_epoch_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MRE Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(num_epochs):\n",
    "# \t# Initialize epoch metrics for each model\n",
    "# \tepoch_mae = [0.] * len(models)\n",
    "# \tepoch_mre = [0.] * len(models)\n",
    "# \tepoch_mse = [0.] * len(models)\n",
    "\t\n",
    "# \tfor X_batch, y_batch in train_loader:\n",
    "# \t\tX_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\t\t\n",
    "# \t\t# Loop over each model\n",
    "# \t\tfor i, model in enumerate(models):\n",
    "# \t\t\tmodel.train()\n",
    "\t\t\t\n",
    "# \t\t\t# Forward pass\n",
    "# \t\t\toutputs = model(X_batch, X_batch[:, 0])\n",
    "# \t\t\toutputs = outputs[:, 0] + outputs[:, 1] / X_batch[:, 0]\n",
    "# \t\t\ty = y_batch[:, 0]\n",
    "\t\t\t\n",
    "# \t\t\t# Calculate losses\n",
    "# \t\t\tmse_loss = F.mse_loss(outputs, y)\n",
    "# \t\t\tmae_loss = F.l1_loss(outputs, y)\n",
    "# \t\t\trelative_errors = torch.abs(outputs - y) / (y + 1e-8)\n",
    "# \t\t\tmre_loss = relative_errors.mean()\n",
    "\t\t\t\n",
    "# \t\t\t# Backpropagation\n",
    "# \t\t\toptimizers.zero_grad()\n",
    "# \t\t\tmre_loss.backward()\n",
    "# \t\t\toptimizers.step()\n",
    "\t\t\t\n",
    "# \t\t\t# Accumulate losses for this model\n",
    "# \t\t\tepoch_mse += mse_loss.item()\n",
    "# \t\t\tepoch_mae += mae_loss.item()\n",
    "# \t\t\tepoch_mre += mre_loss.item()\n",
    "\t\n",
    "# \tprint(f\"[ {datetime.now().strftime(\"%H:%M:%S\")} ] ----- Epoch [{epoch+1}/{num_epochs}] {'-'*150}\")\n",
    "# \t# Average metrics for each model\n",
    "# \tfor i in range(len(models)):\n",
    "# \t\tavg_epoch_mse = epoch_mse / len(train_loader)\n",
    "# \t\tavg_epoch_mae = epoch_mae / len(train_loader)\n",
    "# \t\tavg_epoch_mre = epoch_mre / len(train_loader)\n",
    "\t\t\n",
    "# \t\tprint(f\"{models.name:<50} | MSE: {avg_epoch_mse:<25} | MAE: {avg_epoch_mae:<25} | MRE: {avg_epoch_mre:<25} |\")\n",
    "\t\t\n",
    "# \t\t# Scheduler step\n",
    "# \t\tschedulers.step(avg_epoch_mre)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(num_epochs):\n",
    "# \t# Initialize epoch metrics for each model\n",
    "# \tepoch_mae = [0.] * len(models)\n",
    "# \tepoch_mre = [0.] * len(models)\n",
    "# \tepoch_mse = [0.] * len(models)\n",
    "\t\n",
    "# \tfor X_batch, y_batch in train_loader:\n",
    "# \t\tX_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\t\t\n",
    "# \t\t# Loop over each model\n",
    "# \t\tfor i, model in enumerate(models):\n",
    "# \t\t\tmodel.train()\n",
    "\t\t\t\n",
    "# \t\t\t# Forward pass\n",
    "# \t\t\toutputs = model(X_batch, X_batch[:, 0])\n",
    "# \t\t\toutputs = outputs[:, 0] + outputs[:, 1] / X_batch[:, 0]\n",
    "# \t\t\ty = y_batch[:, 0]\n",
    "\t\t\t\n",
    "# \t\t\t# Calculate losses\n",
    "# \t\t\tmse_loss = F.mse_loss(outputs, y)\n",
    "# \t\t\tmae_loss = F.l1_loss(outputs, y)\n",
    "# \t\t\trelative_errors = torch.abs(outputs - y) / (y + 1e-8)\n",
    "# \t\t\tmre_loss = relative_errors.mean()\n",
    "\t\t\t\n",
    "# \t\t\t# Composite loss\n",
    "# \t\t\tcomposite_loss = (weights_mse * mse_loss + \n",
    "# \t\t\t\t\t\t\t  weights_mae * mae_loss + \n",
    "# \t\t\t\t\t\t\t  weights_mre * mre_loss)\n",
    "\t\t\t\n",
    "# \t\t\t# Backpropagation\n",
    "# \t\t\toptimizers.zero_grad()\n",
    "# \t\t\tcomposite_loss.backward()\n",
    "# \t\t\toptimizers.step()\n",
    "\t\t\t\n",
    "# \t\t\t# Accumulate losses for this model\n",
    "# \t\t\tepoch_mse += mse_loss.item()\n",
    "# \t\t\tepoch_mae += mae_loss.item()\n",
    "# \t\t\tepoch_mre += mre_loss.item()\n",
    "\t\n",
    "# \tprint(f\"[ {datetime.now().strftime(\"%H:%M:%S\")} ] ----- Epoch [{epoch+1}/{num_epochs}] {'-'*150}\")\n",
    "# \t# Compute average metrics for each model\n",
    "# \tfor i in range(len(models)):\n",
    "# \t\tavg_epoch_mse = epoch_mse / len(train_loader)\n",
    "# \t\tavg_epoch_mae = epoch_mae / len(train_loader)\n",
    "# \t\tavg_epoch_mre = epoch_mre / len(train_loader)\n",
    "\t\t\n",
    "# \t\tprint(f\"{models.name:<50} | MSE: {avg_epoch_mse:<25} | MAE: {avg_epoch_mae:<25} | MRE: {avg_epoch_mre:<25} |\")\n",
    "\t\t\n",
    "# \t\t# Update weights for loss adjustment\n",
    "# \t\tweights_mse = avg_epoch_mse / target_loss\n",
    "# \t\tweights_mae = avg_epoch_mae / target_loss\n",
    "# \t\tweights_mre = avg_epoch_mre / target_loss\n",
    "\t\t\n",
    "# \t\t# Scheduler step\n",
    "# \t\tschedulers.step(composite_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f\"models/{model.name}_mae_52.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Evaluation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_path = 'models'\n",
    "models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for model_name in [m for m in os.listdir(models_path) if m.endswith('pth')]:\n",
    "\tprint(model_name)\n",
    "\t# model = BlackScholesNet(input_size=5, hidden_size=128, output_size=2)\n",
    "\t# model.load_state_dict(torch.load(os.path.join(models_path, model_name)))\n",
    "\t# model.to(device)\n",
    "\n",
    "\t# models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BlackScholesNet(input_size=5, hidden_size=128, output_size=2)\n",
    "model.load_state_dict(torch.load(f\"models\\\\black.pth\"))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_samples = 10000000\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_static_test_data(scaler, num_test_samples, 'static_test_data_5.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\serpo\\AppData\\Local\\Temp\\ipykernel_19576\\2451699335.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  X_test_tensor, y_test_tensor = torch.load(file_name)\n"
     ]
    }
   ],
   "source": [
    "X_test_tensor, y_test_tensor = load_static_test_data('static_test_data_5.pt')\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "KAN Black Scholes                                  Results | MSE: 0.00019835337972704247    | MAE: 0.15416928065790636       | Max AE: 0.43278967416525643       | MRE: 743518.5907223832         | Max RE: 241826836.28083214       \n"
     ]
    }
   ],
   "source": [
    "# Initialize metrics for each model\n",
    "test_losses = 0.\n",
    "test_maes = 0.\n",
    "test_max_aes = 0.\n",
    "test_mres = 0.\n",
    "test_max_res = 0.\n",
    "cnt = 0\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.inference_mode():\n",
    "\tfor X_batch, y_batch in test_loader:\n",
    "\t\tX_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\t\t# X_batch = X_batch.unsqueeze(1)  # Add a dimension for sequence length\n",
    "\n",
    "\t\t# Forward pass\n",
    "\t\toutputs = model(X_batch)\n",
    "\t\t# outputs = (outputs[:, 0] + outputs[:, 1]) / 2\n",
    "\t\toutputs = outputs[:, 0]\n",
    "\t\t\n",
    "\t\ty = y_batch[:, 0]\n",
    "\n",
    "\t\t# Mean Squared Error (MSE)\n",
    "\t\tmse_loss = F.mse_loss(outputs, y)\n",
    "\t\ttest_losses += mse_loss.item()\n",
    "\n",
    "\t\t# Mean Absolute Error (MAE)\n",
    "\t\tabs_errors = torch.abs(outputs - y)\n",
    "\t\ttest_maes += abs_errors.sum().item()\n",
    "\n",
    "\t\t# Maximum Absolute Error (Max AE)\n",
    "\t\tmax_ae = abs_errors.max().item()\n",
    "\t\ttest_max_aes = max(test_max_aes, max_ae)\n",
    "\n",
    "\t\t# Mean Relative Error (MRE)\n",
    "\t\tmask = y >= 1e-10\n",
    "\t\ty_m = y[mask]\n",
    "\t\trelative_errors = torch.abs(outputs[mask] - y_m ) / y_m\n",
    "\n",
    "\t\t# Calculate MRE\n",
    "\t\ttest_mres += relative_errors.sum().item()\n",
    "\t\tcnt += len(relative_errors)\n",
    "\n",
    "\t\t# Calculate max relative error\n",
    "\t\ttest_max_res = max(test_max_res, relative_errors.max().item())\n",
    "\n",
    "\n",
    "# Calculate the average metrics over all test samples for each model\n",
    "avg_test_loss = test_losses / len(test_loader.dataset)\n",
    "avg_test_mae = test_maes / len(test_loader.dataset)\n",
    "avg_test_mre = test_mres / cnt\n",
    "\n",
    "print('-'*250)\n",
    "print(f\"{model.name:<50} Results | MSE: {avg_test_loss:<25} | MAE: {avg_test_mae:<25} | Max AE: {test_max_aes:<25} | MRE: {avg_test_mre:<25} | Max RE: {test_max_res:<25}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Black Model                                        Results | MSE: 1.860992115070968e-09     | MAE: 0.0004510874317589473     | Max AE: 0.015712605802571444      | MRE: 3039.6012895961358        | Max RE: 24053090.766225673       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Black Model                                        Results | MSE: 4.3176854823214384e-08    | MAE: 0.002548539893560493      | Max AE: 0.02216988133309572       | MRE: 52930.97271546223         | Max RE: 74994568.97747828        \n",
    "\n",
    "\n",
    "weight decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Black Model\t\t\t\t\t\t\t\t\t\tResults | MSE: 2.3365065048548754e-08\t| MAE: 0.0019228472067950922\t | Max AE: 0.020785850080788537\t  | MRE: 19540.95551665145\t\t | Max RE: 38428277.40361217\t\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Black Deep Model\t\t\t\t\t\t\t\t   Results | MSE: 1.2292241128713081e-08\t| MAE: 0.0014149500863335678\t | Max AE: 0.026046665725385276\t  | MRE: 15973.972288640323\t\t| Max RE: 13644321.567340782\t   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Black RNN Model\t\t\t\t\t\t\t\t\tResults | MSE: 1.589732937737662e-11\t | MAE: 3.226697698242954e-05\t | Max AE: 0.001995025861357906\t  | MRE: 1103.9241832972975\t\t| Max RE: 15041726.760521403\t   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Black RNN Model\t\t\t\t\t\t\t\t\tResults | MSE: 4.777907308376809e-11\t | MAE: 7.51992950659468e-05\t  | Max AE: 0.0023897778645683085\t | MRE: 2156.166863801692\t\t | Max RE: 16343429.899646066\t   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Black RNN Model\t\t\t\t\t\t\t\t\tResults | MSE: 5.004787422430029e-10\t | MAE: 0.00030045160178942263\t| Max AE: 0.003092261246673578\t  | MRE: 8728.737648890628\t\t | Max RE: 21332958.18205698\t\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Black CNN Model\t\t\t\t\t\t\t\t\tResults | MSE: 1.6229420763649162e-08\t| MAE: 0.0016403877602419335\t | Max AE: 0.02096367339459762\t   | MRE: 39130.069827259045\t\t| Max RE: 49456677.41935457\t\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Black ResNet Model\t\t\t\t\t\t\t\t Results | MSE: 1.357335095783607e-07\t | MAE: 0.004738837337220532\t  | Max AE: 0.021764388623186998\t  | MRE: 147254.71198362616\t\t| Max RE: 146963541.83840838\t   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Black Model\t\t\t\t\t\t\t\t\t\tResults | MSE: 1.6345862189413596e-07\t| MAE: 0.005929486075345317\t  | Max AE: 0.024445273630216757\t  | MRE: 308086.0669714369\t\t | Max RE: 102909205.05644394\t   \n",
    "\n",
    "\n",
    "hub no activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Black Model\t\t\t\t\t\t\t\t\t\tResults | MSE: 6.311032924102108e-08\t | MAE: 0.0024049330370174466\t | Max AE: 0.0486327963291493\t\t| MRE: 0.17257685070773462\t   | Max RE: 13.89843139038193\t\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Black Model\t\t\t\t\t\t\t\t\t\tResults | MSE: 4.767372257527874e-09\t | MAE: 0.0008842677848565959\t | Max AE: 0.010480693476281389\t  | MRE: 8610.273830920263\t\t | Max RE: 24765156.665712476   \n",
    "</br>\n",
    "delta 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Black Model\t\t\t\t\t\t\t\t\t\tResults | MSE: 1.7072262899170862e-08\t| MAE: 0.0014622379697655967\t | Max AE: 0.012304333512939192\t  | MRE: 19133.356655940905\t\t| Max RE: 32084991.04810058\t\t\n",
    "</br>\n",
    "Huber loss delta = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Black Model\t\t\t\t\t\t\t\t\t\tResults | MSE: 6.5584848876120205e-09\t| MAE: 0.0010204205638990932\t | Max AE: 0.01436321507469207\t   | MRE: 21704.53521771374\t\t | Max RE: 39553088.01572265\n",
    "</br>\n",
    "Huber loss delta = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Black Model\t\t\t\t\t\t\t\t\t\tResults | MSE: 9.577022434796093e-09\t | MAE: 0.001128070044492659\t  | Max AE: 0.01750700016971668\t   | MRE: 0.0\t\t\t\t\t   | Max RE: 0.0   \n",
    "with grad norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Black Model\t\t\t\t\t\t\t\t\t\tResults | MSE: 3.437281101835667e-08\t | MAE: 0.0014345937355705574\t | Max AE: 0.026702327077854637\t  | MRE: 0.0\t\t\t\t\t   | Max RE: 0.0\t\t  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Black Model\t\t\t\t\t\t\t\t\t\tResults | MSE: 4.870335576499605e-09\t | MAE: 0.0006644182282886656\t | Max AE: 0.02035231469468729\t   | MRE: 0.0\t\t\t\t\t   | Max RE: 0.0\t "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Black Model\t\t\t\t\t\t\t\t\t\tResults | MSE: 3.0745519796730916e-09\t| MAE: 0.0006669217488797097\t | Max AE: 0.01045388565348454\t   | MRE: 0.0\t\t\t\t\t   | Max RE: 0.0\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Black Model\t\t\t\t\t\t\t\t\t\tResults | MSE: 5.394526925641141e-09\t | MAE: 0.0007469986503670395\t | Max AE: 0.024494726553518364\t  | MRE: 0.0\t\t\t\t\t   | Max RE: 0.0   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Black Model\t\t\t\t\t\t\t\t\t\tResults | MSE: 9.944437335789435e-10\t | MAE: 0.0002790903619311921\t | Max AE: 0.020291466710218475\t  | MRE: 0.0\t\t\t\t\t   | Max RE: 5.520420072604463e+31\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "Base Model\t\t\t\t\t\t\t\t\t\t Results | MSE: 3.700808076204782e-09\t | MAE: 0.0006494232504191063\t | Max AE: 0.02520344930434637\t   | MRE: 207.39712310149338\t\t| Max RE: 115508.51118180675 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.distributions import Normal\n",
    "\n",
    "def d1(K, T, sigma, F):\n",
    "\treturn (torch.log(F / K) + (0.5 * sigma**2) * T) / (sigma * torch.sqrt(T))\n",
    "\n",
    "def d2(d1, T, sigma):\n",
    "\treturn d1 - sigma * torch.sqrt(T)\n",
    "\n",
    "def delta(d1, F=1, option_type='call'):\n",
    "\tnormal_dist = Normal(0, 1)\n",
    "\n",
    "\tif option_type == 'call':\n",
    "\t\treturn normal_dist.cdf(d1)\n",
    "\telif option_type == 'put':\n",
    "\t\treturn normal_dist.cdf(d1) - 1\n",
    "\telse:\n",
    "\t\traise ValueError(\"Option type must be 'call' or 'put'\")\n",
    "\n",
    "def gamma(T, sigma, d1, F=1):\n",
    "\tnormal_dist = Normal(0, 1)\n",
    "\tpdf_d1 = torch.exp(normal_dist.log_prob(d1)) \n",
    "\n",
    "\treturn pdf_d1 / (F * sigma * torch.sqrt(T))\n",
    "\n",
    "def theta(K, T, sigma, d1, d2, F=1, r=0, option_type='call'):\n",
    "\tnormal_dist = Normal(0, 1)\n",
    "\tpdf_d1 = torch.exp(normal_dist.log_prob(d1)) \n",
    "\n",
    "\tif option_type == 'call':\n",
    "\t\treturn (-F * pdf_d1 * sigma / (2 * torch.sqrt(T)) - r * K * torch.exp(-r * T) * normal_dist.cdf(d2))\n",
    "\telif option_type == 'put':\n",
    "\t\treturn (-F * pdf_d1 * sigma / (2 * torch.sqrt(T)) + r * K * torch.exp(-r * T) * normal_dist.cdf(-d2))\n",
    "\telse:\n",
    "\t\traise ValueError(\"Option type must be 'call' or 'put'\")\n",
    "\n",
    "def vega(T, d1, F=1):\n",
    "\tnormal_dist = Normal(0, 1)\n",
    "\tpdf_d1 = torch.exp(normal_dist.log_prob(d1))\n",
    "\t\n",
    "\treturn F * pdf_d1 * torch.sqrt(T)\n",
    "\n",
    "def greeks(K, T, sigma, F=1, r=0, option_type='call'):\n",
    "\tdp = d1(K, T, sigma, F)\n",
    "\tdm = d2(dp, T, sigma)\n",
    "\t\n",
    "\treturn delta(dp, F, option_type), gamma(T, sigma, dp, F), theta(K, T, sigma, dp, dm, F, r, option_type), vega(T, dp, F)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10000\n",
    "K = np.random.uniform(1, 2.5, num_samples)\n",
    "T = np.random.uniform(0.004, 4, num_samples)\n",
    "sigma = np.random.uniform(0.1, 0.5, num_samples)\n",
    "\n",
    "# Подготовка данных\n",
    "X = np.vstack((K, T, np.log(K), sigma * np.sqrt(T), sigma**2 * T)).T\n",
    "# X = scaler.fit_transform(X)\n",
    "X_tensor = torch.tensor(X, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "prices = torch.tensor(black_model(1, K, T, sigma), dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.1123328 , 0.17447376, 0.74779293, 0.17846916, 0.03185124],\n",
       "       [2.32165281, 3.31470446, 0.84227935, 0.28785647, 0.08286135],\n",
       "       [1.69476981, 1.0002331 , 0.52754693, 0.47864129, 0.22909749],\n",
       "       ...,\n",
       "       [2.38137775, 1.52928014, 0.86767921, 0.1947643 , 0.03793313],\n",
       "       [1.30069654, 0.52201047, 0.26289992, 0.14615633, 0.02136167],\n",
       "       [1.14877841, 3.78935869, 0.13869913, 0.91709289, 0.84105936]])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KAN Black Scholes\n",
      "Mean Squared Error (MSE): 1.338610235951651e-06\n",
      "Mean Absolute Error (MAE): 0.0004891456605618326\n",
      "Max Absolute Error (MAE): 0.039436549035213875\n",
      "Mean Relative Error (MRE): 2306.777969088586\n",
      "Max Relative Error (MRE): 849233.3873387466\n"
     ]
    }
   ],
   "source": [
    "# Входные данные для модели\n",
    "y = model(X_tensor)\n",
    "# y = (y[:, 0] + y[:, 1] ) / 2\n",
    "y = y[:, 0]\n",
    "\n",
    "abs_errors = torch.abs(y - prices)\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "mse = F.mse_loss(y, prices).item()\n",
    "\n",
    "# Mean Absolute Error (MAE)\n",
    "mae = F.l1_loss(y, prices)\n",
    "\n",
    "# Mean Relative Error (MRE)\n",
    "mask = prices >= 1e-10\n",
    "y_m = prices[mask]\n",
    "relative_errors = torch.abs(y[mask] - y_m ) / y_m\n",
    "mre = relative_errors.mean().item()\n",
    "max_mre = relative_errors.max().item()\n",
    "\n",
    "print(model.name)\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae.item()}\")\n",
    "print(f\"Max Absolute Error (MAE): {abs_errors.max().item()}\")\n",
    "print(f\"Mean Relative Error (MRE): {mre}\")\n",
    "print(f\"Max Relative Error (MRE): {max_mre}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for actual in prices:\n",
    "\tfor forecast in y:\n",
    "\t\terror = abs(forecast-actual)/actual\n",
    "\t\tif error>1e-10:\n",
    "\t\t\tprint(f\"{forecast} vs {actual} error = {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = torch.tensor(X, dtype=torch.float64)\n",
    "greeks_result = greeks(X_tensor[:, 0],X_tensor[:, 1], X_tensor[:, 3] / torch.sqrt(X_tensor[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.8280, -0.4822,  0.7609,  ..., -0.4800, -0.5530,  0.4545],\n",
      "       dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_prices = model(X_tensor)\n",
    "predicted_prices.backward(torch.ones_like(predicted_prices), retain_graph=True)\n",
    "deltas = X_tensor.grad[:, 0].clone()\n",
    "print(deltas)\n",
    "\n",
    "X_tensor.grad.zero_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.1788e-01, 2.9324e-02, 2.1129e-01,  ..., 1.3367e-07, 1.4249e-01,\n",
      "        6.2746e-02], dtype=torch.float64, grad_fn=<MulBackward0>)\n",
      "tensor(0.3526, dtype=torch.float64, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(greeks_result[0])\n",
    "\n",
    "print(F.mse_loss(deltas, greeks_result[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Фиксируем seed для повторяемости\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses for current model:\n",
      "Delta:  0.037595336416915356\n",
      "Black Model without activation\n"
     ]
    }
   ],
   "source": [
    "model.zero_grad()  # Сброс градиентов модели\n",
    "\n",
    "y = model(X_tensor, X_tensor[:, 0])\n",
    "# y = (y[:, 0] + y[:, 1] ) / 2\n",
    "y = y[:, 0]\n",
    "\n",
    "y.backward(torch.ones_like(y), retain_graph=True)\n",
    "K_grad = X_tensor.grad[:, 0].clone() \n",
    "# T_grad = X_tensor.grad[:, 1].clone()  \n",
    "# sigma_grad = (X_tensor.grad[:, 3] / torch.sqrt(X_tensor[:, 1])).clone()\n",
    "\n",
    "X_tensor.grad.zero_()\n",
    "\n",
    "y = model(X_tensor, X_tensor[:, 0])\n",
    "# y = (y[:, 0] + y[:, 1] ) / 2\n",
    "y = y[:, 0]\n",
    "y.backward(torch.ones_like(y), retain_graph=True)\n",
    "delta_grad = X_tensor.grad[:, 0].clone()\n",
    "\n",
    "# X_tensor.grad.zero_()\n",
    "\n",
    "# # Вычисление второго градиента (гамма)\n",
    "# y = model(X_tensor, X_tensor[:, 0])\n",
    "# y = (y[:, 0] + y[:, 1] ) / 2\n",
    "# # y = y[:, 0]\n",
    "# y.backward(torch.ones_like(y), retain_graph=True)\n",
    "# delta_grad.backward(torch.ones_like(delta_grad), retain_graph=True)\n",
    "# gamma_grad = X_tensor.grad[:, 0].clone()\n",
    "\n",
    "delta_loss = F.mse_loss(delta_grad, greeks_result[0]).item()\n",
    "# gamma_loss = F.mse_loss(gamma_grad, greeks_result[1]).item()\n",
    "# theta_loss = F.mse_loss(T_grad, greeks_result[2]).item()\n",
    "# vega_loss = F.mse_loss(sigma_grad, greeks_result[3]).item()\n",
    "\n",
    "print(\"Losses for current model:\")\n",
    "print('Delta: ', delta_loss)\n",
    "# print('Gamma: ', gamma_loss)\n",
    "# print('Theta: ', theta_loss)\n",
    "# print('Vega: ', vega_loss)\n",
    "\n",
    "print(model.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Losses for current model:\n",
    "# Delta:  0.023927056489781124\n",
    "# Black Model without activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Losses for current model:\n",
    "# Delta:  0.04097502044021693\n",
    "# Black Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Losses for current model:\n",
    "# Delta:  0.23041457848259805\n",
    "# Black Model\n",
    "# weight decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Losses for current model:\n",
    "# Delta:  0.09470602026672634\n",
    "# Gamma:  0.4605984598995799\n",
    "# Black Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero gradients before starting\n",
    "model.zero_grad()\n",
    "\n",
    "# Forward pass\n",
    "y = model(X_tensor.unsqueeze(1))  # Add sequence dimension for LSTM\n",
    "y = y[:, 0]  # Get the first output\n",
    "\n",
    "# Delta Calculation (First Derivative)\n",
    "y.backward(torch.ones_like(y), retain_graph=True)  # Backpropagate\n",
    "K_grad = X_tensor.grad[:, 0].clone()  # Gradient w.r.t. K (Delta)\n",
    "X_tensor.grad.zero_()  # Clear gradients for the next step\n",
    "\n",
    "# Gamma Calculation (Second Derivative)\n",
    "y = model(X_tensor.unsqueeze(1))  # Forward pass again\n",
    "y = y[:, 0]\n",
    "y.backward(torch.ones_like(y), retain_graph=True)\n",
    "delta_grad = X_tensor.grad[:, 0].clone().requires_grad_(True)  # Delta gradient\n",
    "\n",
    "X_tensor.grad.zero_()\n",
    "\n",
    "# Now backpropagate Delta to calculate Gamma\n",
    "delta_grad.backward(torch.ones_like(delta_grad), retain_graph=True)\n",
    "gamma_grad = X_tensor.grad[:, 0].clone()  # Gamma\n",
    "\n",
    "# Calculate MSE loss for Delta and Gamma against target Greeks\n",
    "delta_loss = F.mse_loss(delta_grad, greeks_result[0]).item()  # Replace with actual target values for Delta\n",
    "gamma_loss = F.mse_loss(gamma_grad, greeks_result[1]).item()  # Replace with actual target values for Gamma\n",
    "\n",
    "print(\"Losses for the RNN model:\")\n",
    "print('Delta Loss: ', delta_loss)\n",
    "print('Gamma Loss: ', gamma_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Losses for current model:\n",
    "# Delta:  0.07544267106592446\n",
    "# Gamma:  0.4395989266865998\n",
    "# Black Deep Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Losses for the RNN model:\n",
    "# Delta Loss:  0.08094750341367543\n",
    "# Gamma Loss:  0.4721712001137761"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Losses for current model:\n",
    "# Delta:  0.03421519741554771\n",
    "# Gamma:  0.37361078598119973\n",
    "# Black ResNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Losses for current model:\n",
    "# Delta:  0.02984523250068021\n",
    "# Gamma:  0.32884577927187386 hub no act "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
